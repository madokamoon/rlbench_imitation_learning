import numpy as np
import os
from pprint import pprint
import yaml
import importlib
import json
import pathlib
from PIL import Image
import time, datetime
from tqdm import tqdm 
import random
import torch
import pickle
import gc  
import matplotlib.pyplot as plt
import sys 

from rlbench.action_modes.action_mode import MoveArmThenGripper
from rlbench.action_modes.arm_action_modes import JointVelocity, EndEffectorPoseViaPlanning
from rlbench.action_modes.gripper_action_modes import GripperJointPosition
from rlbench.environment import Environment
from rlbench.observation_config import ObservationConfig
from act_policy_wrapper import ACTPolicyWrapper
from pyrep.backend import sim

class RLBenchProcessor:
    """ç”¨äºRLBenchç¯å¢ƒçš„æ•°æ®é‡‡æ ·ä¸è½¨è¿¹æ‰§è¡Œçš„ç»¼åˆå¤„ç†å™¨"""
    
    def __init__(self, config):

        self.mode = config.get('mode', 0)
        print(f"å½“å‰æ¨¡å¼: {self.mode} (0=é‡‡æ ·, 1=è½¨è¿¹å¤ç°, 2=è¯„ä¼°)")

        # ä»é…ç½®ä¸­è·å–å‚æ•°
        data_sampler_config = config.get('data_sampler_config', {})
        self.data_sampler_config = data_sampler_config
        self.save_path_head = data_sampler_config['save_path_head']
        self.save_path_end = data_sampler_config['save_path_end']
        self.taskname = data_sampler_config['taskname']
        self.num_demos = data_sampler_config['num_demos']
        self.image_width = data_sampler_config['image']['width']
        self.image_height = data_sampler_config['image']['height']
        self.camera_names = data_sampler_config['cameras']
        self.static_positions = data_sampler_config['static_positions']
        self.headless = data_sampler_config['headless']
        self.robot_init_state = data_sampler_config['robot_init_state']

        # ä¿å­˜è·¯å¾„
        task_path = os.path.join(self.save_path_head, self.taskname)
        if self.save_path_end == "":
            now_time = datetime.datetime.now()
            str_time = now_time.strftime("%Y-%m-%d-%H-%M-%S")
            self.variation_path = os.path.join(task_path, str_time) 
            self.save_path_end = str_time
        else:
            self.variation_path = os.path.join(task_path, self.save_path_end)
   
        if self.mode == 2:
            act_policy_config = config.get('act_policy')
            self.camera_names_forward = act_policy_config['task_config']['camera_names']
            self.use_weight = act_policy_config['use_weight']
            self.max_steps =  act_policy_config['task_config']['episode_len']
            self.act_policy = ACTPolicyWrapper(act_policy_config)


        self.env = None
        self.task = None
        self.obs_config = None

    def _check_and_make(self, directory):
        if not os.path.exists(directory):
            os.makedirs(directory)
            print(f"åˆ›å»ºç›®å½•: {directory}")
        else :
            user_input = input("è¾“å‡ºè·¯å¾„å·²ç»å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–ï¼Ÿ(y/n): ")
            if user_input.lower() == 'y':
                import shutil
                shutil.rmtree(directory)
                print(f"å·²åˆ é™¤ç°æœ‰ç›®å½•: {directory}")
                os.makedirs(directory)
                print(f"åˆ›å»ºç›®å½•: {directory}")
            else:
                print("æ“ä½œå·²å–æ¶ˆ")
                sys.exit(0) 
                return

            
    def _check_path_exists(self, directory):
        """æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨"""
        if not os.path.exists(directory):
            raise FileNotFoundError(f"è·¯å¾„ä¸å­˜åœ¨: {directory}")
        return True

    def _create_observation_config(self):
        """åˆ›å»ºè§‚æµ‹é…ç½®"""
        obs_config = ObservationConfig()

        cameras_dict = {
            'front_camera': obs_config.front_camera,
            'wrist_camera': obs_config.wrist_camera,
            'left_shoulder_camera': obs_config.left_shoulder_camera,
            'right_shoulder_camera': obs_config.right_shoulder_camera,
            'overhead_camera': obs_config.overhead_camera
        }

        # å…ˆå…³é—­æ‰€æœ‰ç›¸æœº
        for camera in cameras_dict.values():
            camera.rgb = False
            camera.depth = False
            camera.mask = False

        # åªå¯ç”¨é…ç½®ä¸­æŒ‡å®šçš„ç›¸æœº
        for camera_name in self.camera_names:
            if camera_name in cameras_dict:
                cameras_dict[camera_name].rgb = True
                cameras_dict[camera_name].depth = True
                cameras_dict[camera_name].mask = True
                cameras_dict[camera_name].depth_in_meters = True  # å°†æ·±åº¦å­˜å‚¨ä¸º 0-1 å½’ä¸€åŒ–å€¼
                cameras_dict[camera_name].masks_as_one_channel = True  # å°†æ©ç å­˜å‚¨ä¸ºå•é€šé“å›¾åƒ
                cameras_dict[camera_name].image_size = (self.image_width, self.image_height)

        # è®¾ç½®å…¶ä»–è§‚æµ‹å‚æ•°
        obs_config.joint_positions = True
        obs_config.joint_velocities = True
        obs_config.gripper_open = True
        obs_config.gripper_pose = True 
        obs_config.gripper_joint_positions = True
        obs_config.gripper_matrix = True
        obs_config.gripper_touch_forces = True 
        # obs_config.record_gripper_closing = True  # ä¼šå¡å¾ˆä¹…
          
        return obs_config
    
        # æ›´å¤šObservationConfig å¯é…ç½®å‚æ•°ï¼š

        # | å±æ€§å                       | ç±»å‹             | è¯´æ˜                  |
        # | ------------------------- | -------------- | ------------------- |
        # | `front_camera`            | `CameraConfig` | å‰ç½®æ‘„åƒå¤´é…ç½®ï¼ˆRGB/æ·±åº¦/åˆ†å‰²ç­‰ï¼‰ |
        # | `left_shoulder_camera`    | `CameraConfig` | å·¦è‚©æ‘„åƒå¤´é…ç½®             |
        # | `right_shoulder_camera`   | `CameraConfig` | å³è‚©æ‘„åƒå¤´é…ç½®             |
        # | `overhead_camera`         | `CameraConfig` | ä¿¯è§†æ‘„åƒå¤´é…ç½®             |
        # | `wrist_camera`            | `CameraConfig` | æ‰‹è…•æ‘„åƒå¤´é…ç½®             |
        # | `wrist_camera_matrix`     | `bool`         | æ˜¯å¦è¿”å›æ‰‹è…•æ‘„åƒå¤´çš„ 4x4 å˜æ¢çŸ©é˜µ |
        # | `gripper_open`            | `bool`         | è¿”å›å¤¹çˆªæ˜¯å¦å¼ å¼€ï¼ˆå¼€/åˆï¼‰       |
        # | `gripper_pose`            | `bool`         | è¿”å›å¤¹çˆªçš„ä¸–ç•Œä½å§¿ï¼ˆä½ç½® + æ–¹å‘ï¼‰  |
        # | `gripper_joint_positions` | `bool`         | è¿”å›å¤¹çˆªçš„å„ä¸ªå…³èŠ‚è§’åº¦         |
        # | `gripper_matrix`          | `bool`         | è¿”å›å¤¹çˆªçš„ 4x4 å˜æ¢çŸ©é˜µ      |
        # | `gripper_touch_forces`    | `bool`         | è¿”å›å¤¹çˆªçš„è§¦è§‰æ¥è§¦åŠ›é‡         |
        # | `joint_positions`         | `bool`         | è¿”å›æœºæ¢°è‡‚çš„å„ä¸ªå…³èŠ‚ä½ç½®        |
        # | `joint_velocities`        | `bool`         | è¿”å›æœºæ¢°è‡‚çš„å„ä¸ªå…³èŠ‚é€Ÿåº¦        |
        # | `joint_forces`            | `bool`         | è¿”å›æœºæ¢°è‡‚çš„å„ä¸ªå…³èŠ‚å—åŠ›ï¼ˆåŠ›çŸ©ï¼‰    |
        # | `joint_positions_noise`   | `NoiseModel`   | å¯¹å…³èŠ‚ä½ç½®åŠ å…¥å™ªå£°çš„æ¨¡å‹        |
        # | `joint_velocities_noise`  | `NoiseModel`   | å¯¹å…³èŠ‚é€Ÿåº¦åŠ å…¥å™ªå£°çš„æ¨¡å‹        |
        # | `joint_forces_noise`      | `NoiseModel`   | å¯¹å…³èŠ‚åŠ›åŠ å…¥å™ªå£°çš„æ¨¡å‹         |
        # | `task_low_dim_state`      | `bool`         | æ˜¯å¦è¿”å›ä»»åŠ¡çš„ä½ç»´çŠ¶æ€ï¼ˆæ‰‹å·¥ç‰¹å¾ï¼‰   |
        # | `record_gripper_closing`  | `bool`         | æ˜¯å¦è®°å½•å¤¹çˆªé—­åˆè¿‡ç¨‹ï¼ˆç”¨äºæ•°æ®é›†ç”Ÿæˆï¼‰ |




    def task_file_to_task_class(self, task_file):
        class_name = ''.join([w[0].upper() + w[1:] for w in task_file.split('_')])
        mod = importlib.import_module("rlbench.tasks.%s" % task_file)
        mod = importlib.reload(mod)
        task_class = getattr(mod, class_name)
        return task_class,class_name

    def setup_environment(self):
        """è®¾ç½®å¹¶å¯åŠ¨RLBenchç¯å¢ƒ"""
        # åˆ›å»ºè§‚æµ‹é…ç½®
        self.obs_config = self._create_observation_config()
        
        # æ‰“å°è§‚æµ‹é…ç½®ä¿¡æ¯
        print("\n ObservationConfig å±æ€§:")
        pprint(self.obs_config.__dict__)
        print("\n")
        
        if self.mode == 1 or self.mode == 2:
            # ååº”æ¨¡å¼ï¼šä½¿ç”¨æœ«ç«¯ä½å§¿æ§åˆ¶
            action_mode = MoveArmThenGripper(
                arm_action_mode=EndEffectorPoseViaPlanning(),
                gripper_action_mode=GripperJointPosition(absolute_mode=True))
            print("æœ«ç«¯ä½å§¿æ§åˆ¶æ¨¡å¼")
        elif self.mode == 0 :
            # é‡‡æ ·æ¨¡å¼ï¼šä½¿ç”¨å…³èŠ‚é€Ÿåº¦æ§åˆ¶
            action_mode = MoveArmThenGripper(
                arm_action_mode=JointVelocity(),
                gripper_action_mode=GripperJointPosition(absolute_mode=True))
            print("å…³èŠ‚é€Ÿåº¦æ§åˆ¶æ¨¡å¼")
            
        # åˆ›å»ºå¹¶é…ç½®RLBenchç¯å¢ƒ
        self.env = Environment(
            action_mode=action_mode,
            obs_config=self.obs_config, 
            headless=self.headless,
        )
            
        # ä½¿ç”¨simæ¨¡å—ç›´æ¥è®¾ç½®ç‰©ç†å¼•æ“å‚æ•° 
        # sim.simSetEngineFloatParameter()
        # sim.simSetEngineFloatParameter()


        self.env.launch()
        print("rlbenchç¯å¢ƒå¯åŠ¨æˆåŠŸ")

    def load_task(self):
        """åŠ è½½æŒ‡å®šçš„ä»»åŠ¡"""
        print(f"åŠ è½½ä»»åŠ¡: {self.taskname}")
        task_class,classname = self.task_file_to_task_class(self.taskname)
        print(f"åŠ è½½ä»»åŠ¡ç±»: {classname}")
        self.task = self.env.get_task(task_class)

    def save_demo_raw(self, demo, example_path, ex_idx):
        """
        ä¿å­˜æ¼”ç¤ºæ•°æ®ä¸ºåŸå§‹çš„PNGå’ŒJSONæ ¼å¼
        
        Args:
            demo: æ¼”ç¤ºæ•°æ®
            example_path: ä¿å­˜è·¯å¾„
            ex_idx: ç¤ºä¾‹ç´¢å¼•
        """
        # åˆ›å»ºç¤ºä¾‹æ–‡ä»¶å¤¹
        episode_folder = pathlib.Path(example_path).joinpath(f"{ex_idx}")
        episode_folder.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºç›¸æœºæ–‡ä»¶å¤¹
        for cam_name in self.camera_names:
            camera_folder = episode_folder.joinpath(cam_name)
            camera_folder_depth = episode_folder.joinpath(cam_name+"_depth")
            camera_folder_mask = episode_folder.joinpath(cam_name+"_mask")
            camera_folder.mkdir(parents=True, exist_ok=True)
            camera_folder_depth.mkdir(parents=True, exist_ok=True)
            camera_folder_mask.mkdir(parents=True, exist_ok=True)


        # åˆå§‹åŒ–æ•°æ®å­˜å‚¨
        state_data = {}
        
        # ä½¿ç”¨tqdmä¸ºå¾ªç¯æ·»åŠ è¿›åº¦æ¡
        for i, obs in tqdm(enumerate(demo), total=len(demo), desc=f"æ¼”ç¤º {ex_idx} å¤„ç†è¿›åº¦"):
            # è·å–å½“å‰æ—¶é—´æˆ³
            current_timestamp = time.time()
            
            # ä¿å­˜ä¸Šä¸€å¸§çš„åŠ¨ä½œæ•°æ®
            if i > 0:
                # å¯¹äºGripperJointPosition, å°†å¤¹çˆªå¼€åˆåº¦è½¬æ¢ä¸ºå…³èŠ‚ä½ç½®
                prev_frame_data['grasp_action'] = [obs.gripper_open]
                prev_frame_data['robot_joint_action'] = obs.joint_positions.tolist()
                prev_frame_data['robot_action'] = obs.gripper_pose.tolist()
                state_data[str(i-1)] = prev_frame_data

            # å½“å‰å¸§çš„çŠ¶æ€æ•°æ®
            frame_data = {
                'timestamp': current_timestamp,
                'grasp_state': [obs.gripper_open],
                'gripper_joint_positions': obs.gripper_joint_positions.tolist() if obs.gripper_joint_positions is not None else None,
                'robot_joint_state': obs.joint_positions.tolist(),
                'robot_joint_vel': obs.joint_velocities.tolist() if obs.joint_velocities is not None else None,
                'robot_state': obs.gripper_pose.tolist(),
                'robot_state_matrix': obs.gripper_matrix.tolist() if obs.gripper_matrix is not None else None,
            }

            prev_frame_data = frame_data

            # æœ€åä¸€å¸§ä¹Ÿéœ€è¦ä¿å­˜åŠ¨ä½œ
            if i == len(demo) - 1:
                frame_data['grasp_action'] = [obs.gripper_open]
                frame_data['robot_joint_action'] = obs.joint_positions.tolist()
                frame_data['robot_action'] = obs.gripper_pose.tolist()
                state_data[str(i)] = frame_data

            # ä½¿ç”¨å­—å…¸æ˜ å°„ç›¸æœºå±æ€§ååˆ°è§‚æµ‹å¯¹è±¡çš„å±æ€§
            camera_mapping = {
                'front_camera': {
                    'rgb': 'front_rgb',
                    'depth': 'front_depth',
                    'mask': 'front_mask'
                },
                'wrist_camera': {
                    'rgb': 'wrist_rgb',
                    'depth': 'wrist_depth',
                    'mask': 'wrist_mask'
                },
                'left_shoulder_camera': {
                    'rgb': 'left_shoulder_rgb',
                    'depth': 'left_shoulder_depth',
                    'mask': 'left_shoulder_mask'
                },
                'right_shoulder_camera': {
                    'rgb': 'right_shoulder_rgb',
                    'depth': 'right_shoulder_depth',
                    'mask': 'right_shoulder_mask'
                },
                'overhead_camera': {
                    'rgb': 'overhead_rgb',
                    'depth': 'overhead_depth',
                    'mask': 'overhead_mask'
                }
            }

            # éå†æ‰€æœ‰ç›¸æœº
            for camera_name in self.camera_names:
                if camera_name not in camera_mapping:
                    continue

                # ä¿å­˜RGBå›¾åƒ
                rgb_attr = camera_mapping[camera_name]['rgb']
                rgb_img = getattr(obs, rgb_attr, None)
                # print("rgb_img.shape",rgb_img.shape)
                if rgb_img is not None:
                    rgb_path = episode_folder.joinpath(camera_name, f"{i}.png")
                    Image.fromarray(rgb_img).save(str(rgb_path))

                # ä¿å­˜æ·±åº¦å›¾åƒ
                depth_attr = camera_mapping[camera_name]['depth']
                depth_img = getattr(obs, depth_attr, None)
                # print("depth_img.shape",depth_img.shape)
                if depth_img is not None:
                    depth_path = episode_folder.joinpath(f"{camera_name}_depth", f"{i}.png")
                    depth_image = np.clip(depth_img * 100, 0, 255).astype(np.uint8)
                    Image.fromarray(depth_image).save(str(depth_path))

                # ä¿å­˜æ©ç å›¾åƒ
                mask_attr = camera_mapping[camera_name]['mask']
                mask_img = getattr(obs, mask_attr, None)
                # print("mask_img.shape",mask_img.shape)

                if mask_img is not None:

                    # å¤„ç†æ©ç å›¾åƒ
                    # åˆ›å»ºç©ºç™½RGBå›¾åƒ
                    mask_array = mask_img
                    rgb_array = np.zeros((mask_array.shape[0], mask_array.shape[1], 3), dtype=np.uint8)
                    
                    # æ ¹æ®ç°åº¦å€¼è®¾ç½®ä¸åŒçš„RGBå€¼
                    rgb_array[(mask_array == 35) | (mask_array == 31) | (mask_array == 34) , 0] = 255
                    rgb_array[mask_array == 84, 1] = 255
                    rgb_array[mask_array == 83, 2] = 255

                    mask_path = episode_folder.joinpath(f"{camera_name}_mask", f"{i}.png")
                    mask_image = np.clip(rgb_array, 0, 255).astype(np.uint8)
                    Image.fromarray(mask_image).save(str(mask_path))

        # ä¿å­˜çŠ¶æ€æ•°æ®åˆ°JSONæ–‡ä»¶
        state_json_path = episode_folder.joinpath("state.json")
        with open(str(state_json_path), 'w') as json_file:
            json.dump(state_data, json_file, indent=4)
        
        print(f"æ¼”ç¤º {ex_idx} åŸå§‹æ•°æ®ä¿å­˜æˆåŠŸï¼Œè·¯å¾„: {episode_folder}")

    def collect_and_save_demos(self):
        """é€ä¸ªæ”¶é›†ã€ä¿å­˜demoï¼Œå¹¶åœ¨æ¯ä¸ªdemoå¤„ç†å®Œåé‡Šæ”¾å†…å­˜"""
    
        self._check_and_make(self.variation_path)

        config_save_path = os.path.join(self.variation_path, "data_sampler_config.yaml")
        with open(config_save_path, 'w') as f:
            yaml.dump(self.data_sampler_config, f, default_flow_style=False)
    
        print(f"æœ¬æ¬¡è¿è¡Œé…ç½®å·²ä¿å­˜åˆ°: {config_save_path}")

        if self.static_positions == True:

            print("é™æ€ä½ç½®æ¨¡å¼")
            # ç¯å¢ƒçŠ¶æ€ä¿å­˜è·¯å¾„
            env_state_path = os.path.join(self.variation_path, "initial_state.pickle")
            descriptions, first_obs = self.task.reset()
            random_state = random.getstate()
            numpy_state = np.random.get_state()
            reference_demo = self.task.get_demos(1, live_demos=True)[0]
            # åªä¿ç•™ç¬¬ä¸€ä¸ªè§‚å¯Ÿ
            if len(reference_demo._observations) > 1:
                first_obs = reference_demo._observations[0]
                reference_demo._observations = [first_obs]
            # ä¿å­˜ç¯å¢ƒä¿¡æ¯
            env_info = {
                'descriptions': descriptions,
                'random_state': random_state,
                'numpy_state': numpy_state,
                'reference_demo': reference_demo,  # ä¿å­˜å‚è€ƒæ¼”ç¤ºå¯¹è±¡
                'timestamp': datetime.datetime.now().isoformat()
            }
            with open(env_state_path, 'wb') as f:
                pickle.dump(env_info, f)
            
            print(f"åˆå§‹ç¯å¢ƒé…ç½®å·²ä¿å­˜åˆ°: {env_state_path}")

            del reference_demo
            gc.collect() 

            reference_demo, random_state, numpy_state = self.load_reference_demo()

        # é€ä¸ªæ”¶é›†å’Œä¿å­˜demo
        for i in range(self.num_demos):
            print(f'æ”¶é›†å’Œä¿å­˜æ¼”ç¤º {i}/{self.num_demos}')
            
            if self.static_positions == True:
                random.setstate(random_state)
                np.random.set_state(numpy_state)
                self.task.reset_to_demo(reference_demo)
                # æ”¶é›†çš„æ—¶å€™ä¸èƒ½åŠ ä¸Šè¿™å¥ï¼Œå…¶ä»–æƒ…å†µéœ€è¦åŠ ä¸Šï¼ŒåŸå› ä¸º task.get_demos ä¸­ä¼šè°ƒç”¨ reset()
                # descriptions, obs = self.task.reset()  
            else:
                pass
                # descriptions, obs = self.task.reset()

            if self.robot_init_state is not None:
                    self.task.step(self.robot_init_state)

            # åªè·å–ä¸€ä¸ªdemo
            demo = self.task.get_demos(1, live_demos=True)[0]
            
            # ä¿å­˜è¿™ä¸ªdemo
            self.save_demo_raw(demo, self.variation_path, i)
            
            # æ˜¾å¼é‡Šæ”¾å†…å­˜
            del demo
            gc.collect()  

    def load_trajectory(self, trajectory_path):
        """
        ä»æŒ‡å®šè·¯å¾„åŠ è½½è½¨è¿¹æ•°æ®
        
        Args:
            trajectory_path: è½¨è¿¹æ•°æ®è·¯å¾„
            
        Returns:
            dict: åŠ è½½çš„è½¨è¿¹æ•°æ®
        """
        state_file_path = os.path.join(trajectory_path, 'state.json')
        print(f"åŠ è½½è½¨è¿¹æ•°æ®: {state_file_path}")
        
        with open(state_file_path, 'r') as f:
            trajectory_data = json.load(f)
            
        print(f"æˆåŠŸåŠ è½½è½¨è¿¹ï¼Œå…±æœ‰ {len(trajectory_data)} ä¸ªæ•°æ®ç‚¹")
        return trajectory_data

    def execute_trajectory(self, trajectory_data, epoch_idx):
        """
        æ‰§è¡Œè½¨è¿¹æ§åˆ¶
        
        Args:
            trajectory_data: åŒ…å«æœºæ¢°è‡‚è½¨è¿¹çš„å­—å…¸
            epoch_idx: å½“å‰epochçš„ç´¢å¼•
        """

        # æŒ‰é¡ºåºæ‰§è¡Œè½¨è¿¹
        print(f"å¼€å§‹æ‰§è¡Œè½¨è¿¹ (Epoch {epoch_idx})...")
        trajectory_keys = sorted([int(k) for k in trajectory_data.keys()])
        
        for t in tqdm(trajectory_keys, desc=f"Epoch {epoch_idx} æ‰§è¡Œè¿›åº¦"):

            frame_data = trajectory_data[str(t)]
            
            # æå–æœ«ç«¯ä½å§¿å’Œå¤¹çˆªåŠ¨ä½œ
            pose = frame_data['robot_state']  # [x, y, z, qx, qy, qz, qw]
            
            # è·å–å¤¹çˆªå¼€åˆçŠ¶æ€ 
            gripper_open = frame_data.get('grasp_state', [0])[0]
            gripper_joint_position = 0.04 * float(gripper_open > 0.5) 


            # æ‰§è¡ŒåŠ¨ä½œ.
            action = np.array(pose + [gripper_joint_position])
            obs, reward, terminate = self.task.step(action)
            
            # ç®€çŸ­æš‚åœä»¥ä¾¿è§‚å¯Ÿ
            # time.sleep(0.01)
            
            if terminate:
                print(f"Epoch {epoch_idx} ä»»åŠ¡ç»“æŸ")
                break
                
        print(f"è½¨è¿¹ {epoch_idx} æ‰§è¡Œå®Œæˆ")
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œè§‚å¯Ÿæœ€ç»ˆçŠ¶æ€
        time.sleep(1)

    def eval_process_observation(self, obs):

        # æå–å›¾åƒæ•°æ®
        imgdata = {}


        # ä½¿ç”¨å­—å…¸æ˜ å°„ç›¸æœºå±æ€§ååˆ°è§‚æµ‹å¯¹è±¡çš„å±æ€§
        camera_mapping = {
            'front_camera': {
                'rgb': 'front_rgb',
                'depth': 'front_depth',
                'mask': 'front_mask'
            },
            'wrist_camera': {
                'rgb': 'wrist_rgb',
                'depth': 'wrist_depth',
                'mask': 'wrist_mask'
            },
            'left_shoulder_camera': {
                'rgb': 'left_shoulder_rgb',
                'depth': 'left_shoulder_depth',
                'mask': 'left_shoulder_mask'
            },
            'right_shoulder_camera': {
                'rgb': 'right_shoulder_rgb',
                'depth': 'right_shoulder_depth',
                'mask': 'right_shoulder_mask'
            },
            'overhead_camera': {
                'rgb': 'overhead_rgb',
                'depth': 'overhead_depth',
                'mask': 'overhead_mask'
            }
        }

        # éå†æ‰€æœ‰ç›¸æœº
        for camera_name in self.camera_names:
            if camera_name not in camera_mapping:
                continue

            # ä¿å­˜RGBå›¾åƒ
            rgb_attr = camera_mapping[camera_name]['rgb']
            rgb_img = getattr(obs, rgb_attr, None)
            imgdata[camera_name] = rgb_img

            # ä¿å­˜æ©ç å›¾åƒ
            mask_attr = camera_mapping[camera_name]['mask']
            mask_img = getattr(obs, mask_attr, None)

            # å¤„ç†æ©ç å›¾åƒ
            # åˆ›å»ºç©ºç™½RGBå›¾åƒ
            mask_array = mask_img
            rgb_array = np.zeros((mask_array.shape[0], mask_array.shape[1], 3), dtype=np.uint8)
            
            # æ ¹æ®ç°åº¦å€¼è®¾ç½®ä¸åŒçš„RGBå€¼
            rgb_array[(mask_array == 35) | (mask_array == 31) | (mask_array == 34) , 0] = 255
            rgb_array[mask_array == 84, 1] = 255
            rgb_array[mask_array == 83, 2] = 255

            imgdata[f"{camera_name}_mask"] = rgb_array


        import copy

        # å…³èŠ‚æ§åˆ¶
        # robot_state = list(copy.deepcopy(obs.joint_positions)) 
        # robot_state.append(copy.deepcopy(obs.gripper_open))  

        # æœ«ç«¯ä½å§¿æ§åˆ¶
        robot_state = list(copy.deepcopy(obs.gripper_pose)) 
        robot_state.append(copy.deepcopy(obs.gripper_open)) 

        return imgdata, robot_state

    def load_reference_demo(self):   

        reference_demo = None
        random_state = None
        numpy_state = None

        env_state_path = os.path.join(self.variation_path, "initial_state.pickle")
        if env_state_path and os.path.exists(env_state_path):
            print(f"åŠ è½½ç¯å¢ƒçŠ¶æ€æ•°æ®: {env_state_path}")
            try:
                with open(env_state_path, 'rb') as f:
                    env_info = pickle.load(f)
            
                # è·å–å­˜å‚¨çš„çŠ¶æ€
                random_state = env_info.get('random_state')
                numpy_state = env_info.get('numpy_state')
                reference_demo = env_info.get('reference_demo')  # ç›´æ¥è·å–ä¿å­˜çš„å‚è€ƒæ¼”ç¤º
                descriptions = env_info.get('descriptions')
                
                print(f"æˆåŠŸåŠ è½½ç¯å¢ƒçŠ¶æ€ï¼Œæè¿°: {descriptions}")
                return reference_demo,random_state,numpy_state
        
            except Exception as e:
                print(f"åŠ è½½ç¯å¢ƒçŠ¶æ€å¤±è´¥: {e}")
                env_state_path = None
                return None


    def act_eval(self, max_attempts=20):
        """
        æ‰§è¡ŒæŒ‡å®šä»»åŠ¡ï¼Œå¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•ï¼Œå¹¶ç»Ÿè®¡æˆåŠŸç‡å’Œå¹³å‡æ­¥éª¤æ•°
        
        Args:
            max_attempts: æœ€å¤§å°è¯•æ¬¡æ•°

        Returns:
            tuple: (æˆåŠŸç‡, å¹³å‡æ­¥éª¤æ•°)
        """
        import traceback  # å¯¼å…¥tracebackæ¨¡å—
        from weight.weight import calculate_change_weight

        success_counts = 0  # æˆåŠŸæ¬¡æ•°ç»Ÿè®¡
        successful_steps = []  # è®°å½•æ¯æ¬¡æˆåŠŸå°è¯•çš„æ­¥éª¤æ•°
        attempt = 0
        
        if self.static_positions == True:
            reference_demo,random_state,numpy_state = self.load_reference_demo()

        try:
            while attempt < max_attempts:
                attempt += 1
                print(f"\nå¼€å§‹ç¬¬ {attempt}/{max_attempts} æ¬¡å°è¯•æ‰§è¡Œä»»åŠ¡")
                
                if self.static_positions == True:
                    random.setstate(random_state)
                    np.random.set_state(numpy_state)
                    self.task.reset_to_demo(reference_demo)
                    descriptions, obs = self.task.reset()


                else:
                    descriptions, obs = self.task.reset()


                self.act_policy.reset()
                # ç”¨äºå­˜å‚¨ä¸Šä¸€å¸§çš„å›¾åƒ
                prev_images = None
                
                # æ‰§è¡Œæ§åˆ¶å¾ªç¯
                success_in_this_attempt = False

                # forceslist = []

                for step in tqdm(range(self.max_steps), desc=f"ç¬¬ {attempt} æ¬¡å°è¯•"):
                    # å¤„ç†è§‚å¯Ÿè·å–å›¾åƒå’ŒçŠ¶æ€
                    imgdata, robot_state = self.eval_process_observation(obs)
                    formatted_state = [f"{val:8.5f}" for val in robot_state]
                    print(f"robot_state_:{formatted_state}")

                    if self.use_weight :
                        view_weights = []
                        if prev_images is not None:
                            print("è®¡ç®—è§†è§’å˜åŒ–æƒé‡ï¼š")
                            for cam_name in self.camera_names_forward:
                                if cam_name in imgdata:
                                    curr_img = imgdata[f"{cam_name}_mask"]
                                    prev_img = prev_images[f"{cam_name}_mask"]
                                    # è®¡ç®—å˜åŒ–æƒé‡
                                    weight = calculate_change_weight(prev_img, curr_img)
                                    view_weights.append(weight)
                                    print(f"  - {cam_name}: {weight:.4f}")
                            
                            # å½’ä¸€åŒ–æƒé‡ï¼Œç¡®ä¿æ€»å’Œä¸ºç›¸æœºæ•°é‡ï¼ˆå¹³å‡æƒé‡ä¸º1ï¼‰
                            total_weight = sum(view_weights)
                            norm_view_weights = [w * len(view_weights) / total_weight for w in view_weights]
                            print(f"å½’ä¸€åŒ–è§†è§’æƒé‡: {[f'{w:.4f}' for w in norm_view_weights]}")
                            actaction = self.act_policy.get_actions(imgdata, robot_state, view_weights=norm_view_weights)
                        else:
                            print("æ²¡æœ‰ä¸Šä¸€å¸§å›¾åƒï¼Œä½¿ç”¨é»˜è®¤æƒé‡")
                            actaction = self.act_policy.get_actions(imgdata, robot_state)
                            
                        # ä¿å­˜å½“å‰å¸§ä½œä¸ºä¸‹ä¸€æ¬¡è¿­ä»£çš„ä¸Šä¸€å¸§
                        prev_images = {}
                        for cam_name in self.camera_names_forward:
                            if cam_name in imgdata:
                                prev_images[f"{cam_name}_mask"] = imgdata[f"{cam_name}_mask"].copy()

                    else:
                        actaction = self.act_policy.get_actions(imgdata, robot_state)
                        

                    # æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºæœ«ç«¯ä½å§¿æ§åˆ¶
                    end_effector_pose = actaction[0:7].copy()

                    # å•ä½åŒ–å››å…ƒæ•°
                    quat = end_effector_pose[3:7]
                    norm = np.linalg.norm(quat)
                    if norm < 1e-6:
                        quat = np.array([0.0, 0.0, 0.0, 1.0])
                    else:
                        quat = quat / norm
                    end_effector_pose[3:7] = quat

                    # å¤¹çˆªæ§åˆ¶
                    gripper_value = actaction[7]
                    gripper_joint_position = 0.04 * float(gripper_value > 0.5)

                    # æ‰§è¡ŒåŠ¨ä½œ
                    try:
                        action = np.concatenate([end_effector_pose, [gripper_joint_position]]).astype(np.float32)
                        formatted_action = [f"{val:8.5f}" for val in action]
                        print(f"robot_action:{formatted_action}")
                        obs, reward, terminate = self.task.step(action)

                        # forceslist.append(obs.gripper_touch_forces)
                        # formatted_forces = [f"{val:8.5f}" for val in obs.gripper_touch_forces]
                        # print(f"touch_forces:{formatted_forces}")



                        # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
                        if reward == 1.0:
                            print(f"\nğŸ‰ ç¬¬ {attempt} æ¬¡å°è¯•ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ! ä½¿ç”¨æ­¥éª¤: {step+1}")
                            success_counts += 1
                            successful_steps.append(step+1)  # è®°å½•æˆåŠŸæ‰€éœ€çš„æ­¥éª¤æ•°
                            success_in_this_attempt = True
                            break

                        if terminate:
                            print(f"\nâŒ ç¬¬ {attempt} æ¬¡å°è¯•è¢«ç»ˆæ­¢")
                            break

                    except Exception as e:
                        print(f"\nç¬¬ {attempt} æ¬¡å°è¯•æ‰§è¡ŒåŠ¨ä½œæ—¶å‘ç”Ÿé”™è¯¯:")
                        traceback.print_exc()  # æ‰“å°å®Œæ•´çš„å †æ ˆè·Ÿè¸ª
                        break
                
                # forces_array = np.array(forceslist)  
                # è·å–åŠ›ä¼ æ„Ÿå™¨æ•°é‡
                # num_sensors = forces_array.shape[1]
                # time_steps = forces_array.shape[0]
                # plt.figure(figsize=(12, 6))
                # for i in range(num_sensors):
                #     plt.plot(range(time_steps), forces_array[:, i], label=f'force {i+1}')
                # plt.grid(True)
                # plt.legend()
                # plt.show()

                if not success_in_this_attempt and attempt < max_attempts:
                    print(f"\nç¬¬ {attempt} æ¬¡å°è¯•æœªæˆåŠŸï¼Œå°†é‡æ–°å°è¯•")
                    time.sleep(1)
            
            # è®¡ç®—ç»Ÿè®¡ç»“æœ
            success_rate = success_counts / attempt if attempt > 0 else 0
            avg_steps = sum(successful_steps) / len(successful_steps) if successful_steps else 0
            
            print(f"\n===== è¯„ä¼°ç»“æœç»Ÿè®¡ =====")
            print(f"- æ€»å°è¯•æ¬¡æ•°: {attempt}")
            print(f"- æˆåŠŸæ¬¡æ•°: {success_counts}")
            print(f"- æˆåŠŸç‡: {success_rate:.2%}")
            print(f"- å¹³å‡æˆåŠŸæ­¥éª¤æ•°: {avg_steps:.2f}")
            if successful_steps:
                print(f"- æœ€å°‘æ­¥éª¤æ•°: {min(successful_steps)}")
                print(f"- æœ€å¤šæ­¥éª¤æ•°: {max(successful_steps)}")
            
            return success_rate, avg_steps

        except Exception as e:
            print(f"ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯:")
            traceback.print_exc()  # æ‰“å°å®Œæ•´çš„å †æ ˆè·Ÿè¸ª
            return 0, 0

        finally:
            # åªåœ¨æ‰€æœ‰å°è¯•ç»“æŸåå…³é—­ç¯å¢ƒ
            if self.env is not None and attempt >= max_attempts:
                self.env.shutdown()
                print("RLBenchç¯å¢ƒå·²å…³é—­")

    def process_all_epochs(self):
        """å¤„ç†æ‰€æœ‰çš„epochï¼ˆéå†æ•°æ®æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å­æ–‡ä»¶å¤¹ï¼‰"""
        print(f"å¤„ç†æ¥è‡ªè·¯å¾„çš„æ‰€æœ‰è½¨è¿¹: {self.variation_path}")
        
        # æ£€æŸ¥åŸºç¡€è·¯å¾„æ˜¯å¦å­˜åœ¨
        self._check_path_exists(self.variation_path)
        
        # è·å–æ‰€æœ‰å­æ–‡ä»¶å¤¹ï¼ˆepochï¼‰
        epoch_folders = []
        for item in os.listdir(self.variation_path):
            item_path = os.path.join(self.variation_path, item)
            # åªå¤„ç†æ–‡ä»¶å¤¹ä¸”ç¡®ä¿æœ‰state.jsonæ–‡ä»¶
            if os.path.isdir(item_path) and os.path.exists(os.path.join(item_path, 'state.json')):
                epoch_folders.append(item)
        
        # ç¡®ä¿æŒ‰ç…§ç¼–å·æ’åº
        epoch_folders = sorted(epoch_folders, key=lambda x: int(x) if x.isdigit() else float('inf'))
        
        if not epoch_folders:
            print(f"è­¦å‘Š: åœ¨ {self.variation_path} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„è½¨è¿¹æ•°æ®")
            return
            
        print(f"æ‰¾åˆ° {len(epoch_folders)} ä¸ªè½¨è¿¹")
        

        if self.static_positions == True:
            reference_demo, random_state, numpy_state = self.load_reference_demo()

        # éå†æ‰§è¡Œæ¯ä¸ªepoch
        for epoch in epoch_folders:
            epoch_path = os.path.join(self.variation_path, epoch)
            print(f"\nå¤„ç†è½¨è¿¹: {epoch_path}")
            
            # åŠ è½½è½¨è¿¹
            trajectory_data = self.load_trajectory(epoch_path)
            
            if self.static_positions == True:
                random.setstate(random_state)
                np.random.set_state(numpy_state)
                self.task.reset_to_demo(reference_demo)
                descriptions, obs = self.task.reset()
            else:
                descriptions, obs = self.task.reset()

            # æ‰§è¡Œè½¨è¿¹
            self.execute_trajectory(trajectory_data, epoch)
            
            # ç­‰å¾…çŸ­æš‚æ—¶é—´ï¼Œå‡†å¤‡ä¸‹ä¸€ä¸ªè½¨è¿¹
            time.sleep(0.5)

    def run(self):
        """è¿è¡Œå¤„ç†å™¨ï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯æ•°æ®é‡‡æ ·è¿˜æ˜¯è½¨è¿¹æ‰§è¡Œ"""
        try:
            self.setup_environment()
            self.load_task()
            
            if self.mode == 0:
                print("ä»¥æ•°æ®é‡‡æ ·æ¨¡å¼è¿è¡Œ...")
                self.collect_and_save_demos()
            elif self.mode == 1:
                print("ä»¥è½¨è¿¹å¤ç°æ¨¡å¼è¿è¡Œ...")
                self.process_all_epochs()
            elif self.mode == 2:
                print("ä»¥è¯„ä¼°æ¨¡å¼è¿è¡Œ...")
                self.act_eval()
                
        finally:
            if self.env is not None:
                self.env.shutdown()
                print("ç¯å¢ƒå·²å…³é—­")




if __name__ == "__main__":

    # ä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºç§å­
    # seed = int(time.time()) 
    # np.random.seed(seed)
    # random.seed(seed)
    # print(f"ä½¿ç”¨éšæœºç§å­: {seed}")'


    import argparse
    parser = argparse.ArgumentParser(description='RLBenchæ•°æ®é‡‡æ ·ä¸è½¨è¿¹æ‰§è¡Œå·¥å…·')
    parser.add_argument('--config', type=str, default=None, help='æŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()

    if os.path.exists(args.config):
        with open(args.config, 'r') as f:
            print(f"ä½¿ç”¨å‘½ä»¤è¡Œé…ç½®æ–‡ä»¶: {args.config}")
            config = yaml.safe_load(f)
    elif os.path.exists('data_sampler_local.yaml'):
        with open('data_sampler_local.yaml', 'r') as f:
            print("ä½¿ç”¨æœ¬åœ°é…ç½®æ–‡ä»¶ data_sampler_local.yaml")
            config = yaml.safe_load(f)
    else:
        with open('data_sampler.yaml', 'r') as f:
            print("ä½¿ç”¨é»˜è®¤é…ç½®æ–‡ä»¶ data_sampler.yaml")
            config = yaml.safe_load(f)

    processor = RLBenchProcessor(config)
    processor.run()


